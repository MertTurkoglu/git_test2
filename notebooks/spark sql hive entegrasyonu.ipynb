{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da207b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.0.3-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20b3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2973c0",
   "metadata": {},
   "source": [
    "yönetilen veritabanları ve tablolar için varsayılan konumum\n",
    "spark-warehouse şuana kadar oluşturduğum hive tabloları burada depolanıyor\n",
    "hive ile çalışırken sparkSesisonu hive desteği ile başlatmamız gerekir\n",
    "Mevcut bir Hive dağıtımına sahip olmayan kullanıcılar yine de Hive desteğini etkinleştirebilir.ki benim şuanda kurduğum sparkta hive dağıtımına sahip değilim\n",
    "warehouse taki veritabanının varsayılan konumunu belirtmek için spark.sql.warehouse.dir kullanıcam bunuda spark-datawarehouse\n",
    "klasörüne eşitlicem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b5447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mert4\\spark-datawarehouse\n"
     ]
    }
   ],
   "source": [
    "warehouse_location=abspath(\"spark-datawarehouse\")#abspath fonksiyonu mevcut bulunduğum klasörü verir yani C:\\Users\\Mert4\\\n",
    "print(warehouse_location)\n",
    "spark=SparkSession.builder.appName(\"spark sql hive integraton\")\\\n",
    "                            .config(\"spark.sql.warehouse.dir\",warehouse_location)\\\n",
    "                            .enableHiveSupport().getOrCreate()\n",
    "#abspathi kullanmama gerek yok çünkü spark -datawarehouse klasörü ile bu notebook aynı klasörde\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e399f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|    hive1|      false|\n",
      "| default|    hive2|      false|\n",
      "| default|    hive3|      false|\n",
      "| default|   permat|      false|\n",
      "| default|      src|      false|\n",
      "| default|   tablo2|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT,value STRING) USING hive\")\n",
    "#spark.sql(\"LOAD DATA LOCAL INPATH 'kv1.txt' INTO TABLE src\")\n",
    "#spark.sql(\"Truncate  TABLE src\")\n",
    "spark.sql(\"SELECT * FROM src\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22f4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bunu kendim yapmak istedim\n",
    "#aynı txt dosyasını sc.textFile ile okuyup her kolonun değeri \u0001 ile ayrılıyordu buna göre map ile split ederek \n",
    "#her satırın kolon değerlerini dizi içerisine atıp sonrada map ile her elemana ulaşıp dizi içindeki değerlere \n",
    "# 2 tane değer var 1.sine key 2.sine value kolon ismini row ile verdik artık bunu toDF ile dfe dönüştürebiliriz\n",
    "#dfe dönüştürdekten sonra geçici tablo yada hive tablosunuda dönüştürüp sql atabiliriz\n",
    "sc=spark.sparkContext\n",
    "rdd=sc.textFile(\"kv1.txt\").map(lambda x :(x.split(\"\u0001\"))).map(lambda x :Row(key=x[0],value=x[1]))\n",
    "df=rdd.toDF()\n",
    "df.show()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12861741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     500|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM src\").show() # aggregationları da kullanabiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d98202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  0|val_0|\n",
      "|  0|val_0|\n",
      "|  0|val_0|\n",
      "|  2|val_2|\n",
      "|  4|val_4|\n",
      "|  5|val_5|\n",
      "|  5|val_5|\n",
      "|  5|val_5|\n",
      "|  8|val_8|\n",
      "|  9|val_9|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "sqlDF.show()\n",
    "#sql sorgusunun çıktısı dataframe olarak döndüğü için bu çıktıyı bir değişkene eşitleyip dataFrame API kullanabilirim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fb0545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key=0, value='val_0'),\n",
       " Row(key=0, value='val_0'),\n",
       " Row(key=0, value='val_0'),\n",
       " Row(key=2, value='val_2'),\n",
       " Row(key=4, value='val_4'),\n",
       " Row(key=5, value='val_5'),\n",
       " Row(key=5, value='val_5'),\n",
       " Row(key=5, value='val_5'),\n",
       " Row(key=8, value='val_8'),\n",
       " Row(key=9, value='val_9')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfobjesi.rdd df i rdd ye dönüştürür\n",
    "dfToRdd=sqlDF.rdd\n",
    "dfToRdd.collect()\n",
    "#rdd yi dfe çevirmek için rdd nin çıktısı aşağıdaki gibi olması lazım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757095ce",
   "metadata": {},
   "source": [
    "sparksql yapılandırılmış veriyi işlemek için bir spark modülüdür.\n",
    "Spark SQL, Apache Hive'da depolanan verilerin okunmasını ve yazılmasını da destekler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2497ab",
   "metadata": {},
   "source": [
    "DataFrame'lerdeki öğeler, her bir sütuna sıraya göre erişmenizi sağlayan Row türündedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaef14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 2, Value: val_2\n",
      "Key: 4, Value: val_4\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 8, Value: val_8\n",
      "Key: 9, Value: val_9\n"
     ]
    }
   ],
   "source": [
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "for record in stringsDS.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c61795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|key| value|\n",
      "+---+------+\n",
      "|  1| val_1|\n",
      "|  2| val_2|\n",
      "|  3| val_3|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  6| val_6|\n",
      "|  7| val_7|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 13|val_13|\n",
      "| 14|val_14|\n",
      "| 15|val_15|\n",
      "| 16|val_16|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "| 20|val_20|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.show()\n",
    "recordsDF.createOrReplaceTempView(\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4d6fa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|key| value|key| value|\n",
      "+---+------+---+------+\n",
      "|  2| val_2|  2| val_2|\n",
      "|  4| val_4|  4| val_4|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  8| val_8|  8| val_8|\n",
      "|  9| val_9|  9| val_9|\n",
      "| 10|val_10| 10|val_10|\n",
      "| 11|val_11| 11|val_11|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 17|val_17| 17|val_17|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 19|val_19| 19|val_19|\n",
      "| 20|val_20| 20|val_20|\n",
      "| 24|val_24| 24|val_24|\n",
      "| 24|val_24| 24|val_24|\n",
      "+---+------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b11f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,101))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe16787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|key| value|\n",
      "+---+------+\n",
      "|  1| val_1|\n",
      "|  2| val_2|\n",
      "|  3| val_3|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  6| val_6|\n",
      "|  7| val_7|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 13|val_13|\n",
      "| 14|val_14|\n",
      "| 15|val_15|\n",
      "| 16|val_16|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "| 20|val_20|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdddf=rdd.map(lambda x :Row (key=x,value=\"val_\"+str(x)))\n",
    "denemedf=rdddf.toDF()\n",
    "denemedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc36aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|key| value|key| value|\n",
      "+---+------+---+------+\n",
      "|  2| val_2|  2| val_2|\n",
      "|  4| val_4|  4| val_4|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  8| val_8|  8| val_8|\n",
      "|  9| val_9|  9| val_9|\n",
      "| 10|val_10| 10|val_10|\n",
      "| 11|val_11| 11|val_11|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 17|val_17| 17|val_17|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 19|val_19| 19|val_19|\n",
      "| 20|val_20| 20|val_20|\n",
      "| 24|val_24| 24|val_24|\n",
      "| 24|val_24| 24|val_24|\n",
      "+---+------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "denemedf.createOrReplaceTempView(\"deneme\")\n",
    "spark.sql(\"SELECT * FROM deneme d JOIN src s ON d.key=s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50b190c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Key: 0, Value: val_0',\n",
       " 'Key: 0, Value: val_0',\n",
       " 'Key: 0, Value: val_0',\n",
       " 'Key: 2, Value: val_2',\n",
       " 'Key: 4, Value: val_4',\n",
       " 'Key: 5, Value: val_5',\n",
       " 'Key: 5, Value: val_5',\n",
       " 'Key: 5, Value: val_5',\n",
       " 'Key: 8, Value: val_8',\n",
       " 'Key: 9, Value: val_9']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringsDS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
