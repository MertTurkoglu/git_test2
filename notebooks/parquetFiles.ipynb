{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c314f2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.0.3-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abed9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "                .appName(\"parquetFiles\")\\\n",
    "                .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cd00a",
   "metadata": {},
   "source": [
    "Apache Spark, veri işleme için büyük veri ortamında sıklıkla kullanılan bir hesaplama motorudur, ancak depolama sağlamaz, bu nedenle tipik bir senaryoda, veri işleme çıktısının harici bir depolama sisteminde depolanması gerekir. Spark SQL, dosya biçimi (CSV, JSON, metin, Parquet, ORC) veya JDBC gibi veri kaynakları için birkaç bağlayıcı sağlar. 2.4'ten beri Apache Avro için de destek vardır ve 3.0'dan beri ikili dosyalar için destek de okur. Ek olarak, bazılarını adlandırmak için MongoDB, ElasticSearch veya DynamoDB gibi diğer veri kaynaklarına bağlanmaya izin veren birkaç kitaplık vardır.\n",
    "Parquet, Avro’dan farklı bir amaca sahiptir. Maksimum şema esnekliğine izin vermek yerine, sorgu hızlarını artırmak ve disk i/o’sunu azaltmak için kullanabileceğiniz şema türlerini optimize etmeye çalışır. Parquet, sütunlarda iç içe tiplere izin vererek geleneksel sütun bazlı tutmanın bazı zayıflıklarının üstesinden gelmeye çalışır. Böylece teknik olarak bir dizi olan bir sütuna ya da aslında birkaç sütun olan bir sütuna sahip olabilirsiniz.\n",
    "Parquet, çok sayıda düşük seviye optimizasyona ve belgelerde bulabileceğiniz diskte nasıl depolandığı ile ilgili bir dizi ayrıntıya sahiptir. Parquet belki de Spark ile ilgili birçok projede göreceğiniz en yaygın dosya formatıdır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877cf9",
   "metadata": {},
   "source": [
    "CSV gibi satır tabanlı dosyalara kıyasla, verilerin verimli bir şekilde sütun halinde depolanması için tasarlanmıştır.\n",
    "sparksql parquet dosyalarını okumamızı ve yazmamızı sağlar\n",
    "columnar formattadır orjinal datanın şemasını korur\n",
    "Çok verimli sıkıştırma işlemi ve şemaları şifrelemeyi desteklemek için tasarlamıştır.\n",
    "verileri sütün bazında depoladığı için alakasız verileri çok hızlı bir şekilde atabilir\n",
    "columnar şekilde verileri depolayarak verilere daha hızlı şekilde ulaşabiliyorum ve donanım tasarrufu sağlayabilirim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d74ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjson=spark.read.load(\"people.json\",format=\"json\")#localimdeki json dosyasını okuyarak df oluşturdum\n",
    "dfjson.write.save(\"people.parquet\",format=\"parquet\",mode =\"ignore\")\n",
    "#df i şema bilgilerini koruyarak parquet dosyası halinde kaydedebilirim\n",
    "parquetFile=spark.read.load(\"people.parquet\",format=\"parquet\")\n",
    "#kaydettiğim people.parquet dosyasını okuyarak dfParquet adında df oluşturdum\n",
    "#şimdi bu dfe sql atabilmek için tablo oluşturmam lazım\n",
    "parquetFile.registerTempTable(\"parquetFile\")\n",
    "spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56baee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee66fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|double|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     2|     4|\n",
      "|     3|     9|\n",
      "|     4|    16|\n",
      "|     5|    25|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|single|triple|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     2|     8|\n",
      "|     3|    27|\n",
      "|     4|    64|\n",
      "|     5|   125|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSquare=spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda x :Row(single=x,double=x**2)))\n",
    "dfSquare.show()\n",
    "dfSquare.write.parquet(\"data/test_table/key=1\",mode=\"ignore\")\n",
    "dfTrible=spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda x :Row(single=x,triple=x**3)))\n",
    "dfTrible.write.parquet(\"data/test_table/key=2\",mode=\"ignore\")\n",
    "dfTrible.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9540d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|single|double|key|\n",
      "+------+------+---+\n",
      "|     1|  null|  2|\n",
      "|     2|  null|  2|\n",
      "|     3|  null|  2|\n",
      "|     4|  null|  2|\n",
      "|     5|  null|  2|\n",
      "|     1|     1|  1|\n",
      "|     2|     4|  1|\n",
      "|     3|     9|  1|\n",
      "|     4|    16|  1|\n",
      "|     5|    25|  1|\n",
      "+------+------+---+\n",
      "\n",
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF=spark.read.parquet(\"data/test_table\")\n",
    "DF.show()\n",
    "DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6bce315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---+\n",
      "|single|double|triple|key|\n",
      "+------+------+------+---+\n",
      "|     1|  null|     1|  2|\n",
      "|     2|  null|     8|  2|\n",
      "|     3|  null|    27|  2|\n",
      "|     4|  null|    64|  2|\n",
      "|     5|  null|   125|  2|\n",
      "|     1|     1|  null|  1|\n",
      "|     2|     4|  null|  1|\n",
      "|     3|     9|  null|  1|\n",
      "|     4|    16|  null|  1|\n",
      "|     5|    25|  null|  1|\n",
      "+------+------+------+---+\n",
      "\n",
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- triple: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedDf=spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDf.show()\n",
    "mergedDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dae65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|single|double|triple|\n",
      "+------+------+------+\n",
      "|     1|     1|     1|\n",
      "|     2|     4|     8|\n",
      "|     3|     9|    27|\n",
      "|     4|    16|    64|\n",
      "|     5|    25|   125|\n",
      "+------+------+------+\n",
      "\n",
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- triple: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSquare.registerTempTable(\"dfSquare\")\n",
    "df3column=spark.sql(\"SELECT single,double,single*single*single as triple FROM dfSquare\")\n",
    "df3column.show()\n",
    "df3column.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85155bdc",
   "metadata": {},
   "source": [
    "spark sql daha iyi performans için parquet metadatasını önbelleğe alır 7\n",
    "aynı şekilde eğer parquet dosyasındaki verileri tabloya çevirirsem tabloyuda önbelleğe alır\n",
    "bu tablolar eğer güncelleniyorsa bunları yenilemek gerekir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.refreshTable(\"my_table\") diyip güncelleyebilirim tabloyu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
